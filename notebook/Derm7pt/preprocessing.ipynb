{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f088c2a",
   "metadata": {},
   "source": [
    "# Preprocessing CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802e3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root_path = os.path.dirname(notebook_dir)\n",
    "sys.path.insert(0, project_root_path)\n",
    "\n",
    "from src.preprocessing.Derm7pt import *\n",
    "from src import ImageConceptDataset\n",
    "from src.preprocessing import *\n",
    "from src.utils import get_paths, load_Derm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30bfd65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from src.config import PROJECT_ROOT\n",
    "\n",
    "print(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebb376a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m paths = \u001b[43mget_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m dataset_handler = load_Derm_dataset(paths)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/career/lab_ujm/test_reproducibility/venv_test/lib/python3.11/site-packages/src/utils/helpers.py:60\u001b[39m, in \u001b[36mget_paths\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_paths\u001b[39m():\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get all paths needed for preprocessing.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     dir_images = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROJECT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDerm7pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     dir_data = os.path.join(PROJECT_ROOT, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDerm7pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     64\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdir_images\u001b[39m\u001b[33m'\u001b[39m: dir_images,\n\u001b[32m     65\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdir_data\u001b[39m\u001b[33m'\u001b[39m: dir_data,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmapping_file\u001b[39m\u001b[33m'\u001b[39m: os.path.join(dir_data, \u001b[33m'\u001b[39m\u001b[33mimage_names.txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     73\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:76\u001b[39m, in \u001b[36mjoin\u001b[39m\u001b[34m(a, *p)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "paths = get_paths()\n",
    "dataset_handler = load_Derm_dataset(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36959f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure text files exist\n",
    "if not os.path.exists(paths['labels_file']):\n",
    "    export_image_props_to_text(dataset_handler.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f333b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e0616",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = dataset_handler.df\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e05963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_labels = dataset_handler.get_labels(data_type='all', one_hot=True)\n",
    "mapping_data = pd.read_csv(paths['mapping_file'], sep=' ', header=None, names=['img_id', 'img_path', 'img_type', 'case_id'])\n",
    "\n",
    "concepts_matrix = []\n",
    "\n",
    "for _, row in mapping_data.iterrows():\n",
    "    case_id = row['case_id']\n",
    "    instance_concepts = None\n",
    "    for concept, vals in all_labels.items():\n",
    "        if concept == 'DIAG':\n",
    "            continue\n",
    "\n",
    "        concept_one_hot = vals[case_id]\n",
    "\n",
    "        if instance_concepts is None:\n",
    "            instance_concepts = concept_one_hot\n",
    "        else:\n",
    "            # Concatenate horizontally (along axis 1)\n",
    "            instance_concepts = np.hstack((instance_concepts, concept_one_hot))\n",
    "\n",
    "    concepts_matrix.append(instance_concepts)\n",
    "\n",
    "concepts_matrix1 = np.array(concepts_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels and concepts\n",
    "image_labels = one_hot_encode_labels(paths['labels_file'], paths['classes_path'], verbose=verbose)\n",
    "concepts_matrix = encode_image_concepts(dataset_handler, paths['mapping_file'], verbose=verbose)\n",
    "\n",
    "# Load and transform images\n",
    "image_tensors, image_paths = load_and_transform_images(paths['dir_images'], paths['mapping_file'], resol=299, use_training_transforms=True, batch_size=32, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter if needed\n",
    "if image_labels.shape[0] != len(image_tensors):\n",
    "    filtered_image_labels, filtered_concepts_matrix = filter_concepts_labels(\n",
    "        paths['mapping_file'], image_tensors, image_paths, image_labels, concepts_matrix\n",
    "    )\n",
    "else:\n",
    "    filtered_image_labels, filtered_concepts_matrix = image_labels, concepts_matrix\n",
    "\n",
    "if verbose:\n",
    "    print(\"Labels shape:\", filtered_image_labels.shape)\n",
    "    print(\"Concepts shape:\", filtered_concepts_matrix.shape)\n",
    "    print(\"Image tensors length:\", len(image_tensors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ea173",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_dict, concepts_dict, labels_dict = split_data_by_indices(\n",
    "    image_tensors, image_paths, filtered_concepts_matrix, filtered_image_labels,\n",
    "    paths, verbose=verbose\n",
    ")\n",
    "\n",
    "train_concept_labels = concepts_dict['train']\n",
    "val_concept_labels = concepts_dict['val']\n",
    "test_concept_labels = concepts_dict['test']\n",
    "\n",
    "train_img_labels = labels_dict['train']\n",
    "val_img_labels = labels_dict['val']\n",
    "test_img_labels = labels_dict['test']\n",
    "\n",
    "train_tensors = tensors_dict['train']\n",
    "val_tensors = tensors_dict['val']\n",
    "test_tensors = tensors_dict['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ead3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concept processing\n",
    "from src.config import DERM7PT_CONFIG\n",
    "\n",
    "class_level_concepts = compute_class_level_concepts(train_concept_labels, None, train_img_labels)\n",
    "\n",
    "# apply class-level concepts to each instance\n",
    "if True:\n",
    "    train_concept_labels, val_concept_labels, test_concept_labels = apply_class_concepts_to_instances(\n",
    "        class_level_concepts, DERM7PT_CONFIG, train_img_labels, train_concept_labels,\n",
    "        test_img_labels, test_concept_labels, val_img_labels, val_concept_labels)\n",
    "\n",
    "common_concept_indices = select_common_concepts(class_level_concepts, min_class_count=0, CUB=False)\n",
    "train_concept_labels = train_concept_labels[:, common_concept_indices]\n",
    "val_concept_labels = val_concept_labels[:, common_concept_indices]\n",
    "test_concept_labels = test_concept_labels[:, common_concept_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c20eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.config import PROJECT_ROOT\n",
    "\n",
    "np.save(os.path.join(PROJECT_ROOT, 'output', 'Derm7pt', 'class_level_concepts.npy'), class_level_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315dabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_concept_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5acdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAIN AND TEST DATASET\n",
    "train_dataset = ImageConceptDataset(\n",
    "    image_tensors=train_tensors,\n",
    "    concept_labels=train_concept_labels,\n",
    "    image_labels=train_img_labels\n",
    ")\n",
    "\n",
    "test_dataset = ImageConceptDataset(\n",
    "    image_tensors=test_tensors,\n",
    "    concept_labels=test_concept_labels,\n",
    "    image_labels=test_img_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b624721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATALOADERS FROM DATASETS\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
