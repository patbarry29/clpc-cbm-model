{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c415bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root_path = os.path.dirname(notebook_dir)\n",
    "sys.path.insert(0, project_root_path)\n",
    "\n",
    "from config import PROJECT_ROOT\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4b59a",
   "metadata": {},
   "source": [
    "# Load and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c995f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C_train = np.load(os.path.join(PROJECT_ROOT, 'output', 'C_train_instance.npy'))\n",
    "# C_hat_train = np.load(os.path.join(PROJECT_ROOT, 'output', 'C_hat_sigmoid_train_instance.npy'))\n",
    "# one_hot_Y_train = np.load(os.path.join(PROJECT_ROOT, 'output', 'Y_train_instance.npy'))\n",
    "\n",
    "# C_test = np.load(os.path.join(PROJECT_ROOT, 'output', 'C_test_instance.npy'))\n",
    "# C_hat_test = np.load(os.path.join(PROJECT_ROOT, 'output', 'C_hat_sigmoid_test_instance.npy'))\n",
    "# one_hot_Y_test = np.load(os.path.join(PROJECT_ROOT, 'output', 'Y_test_instance.npy'))\n",
    "\n",
    "C_hat_train = np.load(os.path.join(PROJECT_ROOT, 'output', 'C_hat_sigmoid_train.npy'))\n",
    "one_hot_Y_train = np.load(os.path.join(PROJECT_ROOT, 'output', 'Y_train.npy'))\n",
    "\n",
    "C_hat_test = np.load(os.path.join(PROJECT_ROOT, 'output', 'C_hat_sigmoid_test.npy'))\n",
    "one_hot_Y_test = np.load(os.path.join(PROJECT_ROOT, 'output', 'Y_test.npy'))\n",
    "\n",
    "class_level_concepts = np.load(os.path.join(PROJECT_ROOT, 'output', 'class_level_concepts.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3226bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.argmax(one_hot_Y_train, axis=1)\n",
    "Y_test = np.argmax(one_hot_Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ad0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_train = []\n",
    "for y in Y_train:\n",
    "    C_train.append(class_level_concepts[y])\n",
    "\n",
    "C_train = np.array(C_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9f675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C_hat_train.shape, one_hot_Y_train.shape, C_hat_test.shape, one_hot_Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6054b78b-b816-4763-87ff-35034efd2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C_hat_train[C_hat_train < 0.1] = 0\n",
    "# C_hat_test[C_hat_test < 0.1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99f9c0",
   "metadata": {},
   "source": [
    "# Classic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424779b",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a6ed058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test accuracy: 0.664998274076631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(C_hat_train, Y_train)\n",
    "print(f\"Logistic Regression Test accuracy: {model.score(C_hat_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e627c",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec9af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Test accuracy: 0.6467034863652054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(C_hat_train, Y_train)\n",
    "print(f\"k-NN Test accuracy: {model.score(C_hat_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14b904",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e8610c3-c78b-4997-9055-10af7ccf5958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Test accuracy: 0.5957887469796341\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(C_hat_train, Y_train)\n",
    "print(f\"Decision Tree Test accuracy: {model.score(C_hat_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff22cb",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9c3248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Test accuracy: 0.6487745944080083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,256, 128), max_iter=1000)\n",
    "mlp.fit(C_hat_train, Y_train)\n",
    "print(f\"MLP Test accuracy: {mlp.score(C_hat_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5950cfd",
   "metadata": {},
   "source": [
    "# Accuracy Using Class-Level Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a37c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall accuracy using concept-based nearest neighbor: 0.6124\n"
     ]
    }
   ],
   "source": [
    "# Function to find the closest concept vector and predict the label\n",
    "def predict_nearest_concept(instance, reference_concepts, reference_labels):\n",
    "    distances = np.sqrt(np.sum((reference_concepts - instance)**2, axis=1))\n",
    "    min_idx = np.argmin(distances)\n",
    "    return reference_labels[min_idx]\n",
    "\n",
    "# Use C_train as reference concepts and evaluate on C_hat_test\n",
    "correct_predictions = 0\n",
    "total_predictions = len(C_hat_test)\n",
    "\n",
    "for i, test_instance in enumerate(C_hat_test):\n",
    "    predicted_label = predict_nearest_concept(test_instance, C_train, Y_train)\n",
    "    true_label = Y_test[i]\n",
    "\n",
    "    if predicted_label == true_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"\\nOverall accuracy using concept-based nearest neighbor: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9dc649-e4d0-40d5-aa23-bd8b5360bea6",
   "metadata": {},
   "source": [
    "# Prototype-Based Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6be3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "118ea801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e479327d-550e-47bd-b1da-54c186acc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PrototypeLearner(nn.Module):\n",
    "#     def forward(self, C_hat, Y_true=None, lambda_bin=0.1, lambda_spars=0.01):\n",
    "#         # Get continuous prototypes\n",
    "#         prototypes = self.prototypes.weight\n",
    "#         prototypes_sigmoid = torch.sigmoid(prototypes)  # Shape: [num_classes, num_concepts]\n",
    "\n",
    "#         # Calculate absolute difference between concepts and prototypes\n",
    "#         concept_distances = torch.abs(C_hat.unsqueeze(1) - prototypes_sigmoid)  # Shape: [batch_size, num_classes, num_concepts]\n",
    "#         # Sum distances across concept dimension\n",
    "#         label_distances = concept_distances.sum(dim=2)  # Shape: [batch_size, num_classes]\n",
    "#         pred_label = label_distances.argmin(dim=1)  # Shape: [batch_size]\n",
    "\n",
    "#         # Classification loss - using the distances for labeled classes\n",
    "#         loss_class = torch.mean(torch.sum(label_distances * Y_true, dim=1))\n",
    "\n",
    "#         # Binarization loss - encourages prototypes to be binary (0 or 1)\n",
    "#         loss_bin = torch.mean(prototypes_sigmoid * (1 - prototypes_sigmoid))\n",
    "\n",
    "#         # Sparsity loss - encourages fewer active concepts\n",
    "#         loss_spars = torch.mean(torch.abs(prototypes_sigmoid))\n",
    "\n",
    "#         # Combine losses\n",
    "#         total_loss = loss_class + (lambda_bin * loss_bin) + (lambda_spars * loss_spars)\n",
    "\n",
    "#         return pred_label, total_loss\n",
    "\n",
    "#     def get_binary_prototypes(self):\n",
    "#         with torch.no_grad():\n",
    "#             binary_prototypes = (torch.sigmoid(self.prototypes.weight) > 0.5).float()\n",
    "#         return binary_prototypes\n",
    "\n",
    "#     def get_sigmoid_prototypes(self):\n",
    "#         return torch.sigmoid(self.prototypes.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba64a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypeClassifier(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.protoypes = nn.Parameter(torch.rand(num_classes, num_features))  # initialize the protptype matrix P\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, num_features)\n",
    "        # L1distance：|x_i - M_m|_1\n",
    "        # (batch_size, num_classes, num_features)\n",
    "        dist = torch.abs(x.unsqueeze(1) - torch.sigmoid(self.protoypes))\n",
    "        dist = dist.sum(dim=2)\n",
    "        return dist  # (batch_size, num_classes)\n",
    "\n",
    "    def binary_regularization(self):\n",
    "        sigmoid_protos = torch.sigmoid(self.protoypes)\n",
    "        return (sigmoid_protos * (1 - sigmoid_protos)).mean()\n",
    "\n",
    "    def sparsity_regularization(self):\n",
    "        return torch.sum(torch.sigmoid(self.protoypes))\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            Prototypes = torch.sigmoid(self.protoypes)\n",
    "            Prototypes[Prototypes>=0.5] = 1\n",
    "            Prototypes[Prototypes<0.5]= 0\n",
    "            dists = torch.abs(x.unsqueeze(1) - Prototypes)\n",
    "            dists = dists.sum(dim=2)\n",
    "            predictions = dists.argmin(dim=1)\n",
    "        return predictions\n",
    "\n",
    "    def get_sigmoid_prototypes(self):\n",
    "        return torch.sigmoid(self.protoypes)\n",
    "\n",
    "    def concept_wise_dist(self, x):\n",
    "        with torch.no_grad():\n",
    "            Prototypes = torch.sigmoid(self.protoypes)\n",
    "            Prototypes[Prototypes>=0.5] = 1\n",
    "            Prototypes[Prototypes<0.5]= 0\n",
    "            dists = x.unsqueeze(1) - Prototypes\n",
    "            # predictions = self.predict(x)\n",
    "            # dists = dists[torch.arange(x.shape[0]), predictions,:]\n",
    "        return dists\n",
    "\n",
    "    def threshold(self, val_x, val_y):\n",
    "        pass\n",
    "\n",
    "    def conformal_predict(self, x):\n",
    "        pass\n",
    "\n",
    "    def explanation(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf1283",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ba7aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, distances, y_true, lambda_binary, lambda_L1):\n",
    "    # label loss\n",
    "    loss_cls = (distances*y_true).sum(axis=1)\n",
    "    loss_cls = loss_cls.mean()\n",
    "\n",
    "    # regularization loss\n",
    "    loss_binary = model.binary_regularization()\n",
    "    loss_sparsity = model.sparsity_regularization()\n",
    "\n",
    "    # total loss\n",
    "    loss = loss_cls + (lambda_binary * loss_binary) + (lambda_L1 * loss_sparsity)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa6615",
   "metadata": {},
   "source": [
    "## Epoch Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab791469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, optimizer, lambda_binary, lambda_L1, device='cpu'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x_batch, y_batch in train_dataloader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        distances = model(x_batch)\n",
    "\n",
    "        # total loss\n",
    "        loss = calculate_loss(model, distances, y_batch, lambda_binary, lambda_L1)\n",
    "\n",
    "        # back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulative training loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # calculate the prediction accuracy\n",
    "        predicted = distances.argmin(dim=1)\n",
    "        real_labels = y_batch.argmax(dim=1)\n",
    "        correct += (predicted == real_labels).sum().item()\n",
    "        total += x_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = correct / total * 100\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b3b0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "def val_epoch(model, val_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            dist = model(x_batch)\n",
    "\n",
    "            predicted = dist.argmin(dim=1)\n",
    "            real_labels = y_batch.argmax(dim=1)\n",
    "            correct += (predicted == real_labels).sum().item()\n",
    "            total += x_batch.size(0)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da7dda",
   "metadata": {},
   "source": [
    "## Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4bc4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Training and Validation Setup ---\n",
    "# C_hat_full_tensor = torch.tensor(C_hat_train, dtype=torch.float32)\n",
    "# Y_full_tensor = torch.tensor(one_hot_Y_train, dtype=torch.long)\n",
    "\n",
    "# # Split data into training and validation\n",
    "# val_split_ratio = 0.2\n",
    "# random_seed = 42\n",
    "\n",
    "# C_hat_train_tensor, C_hat_val_tensor, Y_train_tensor, Y_val_tensor = train_test_split(\n",
    "#     C_hat_full_tensor, Y_full_tensor,\n",
    "#     test_size=val_split_ratio,\n",
    "#     random_state=random_seed,\n",
    "#     stratify=Y_full_tensor\n",
    "# )\n",
    "\n",
    "# # 2. Create DataLoaders\n",
    "# batch_size = 64\n",
    "# train_dataset = TensorDataset(C_hat_train_tensor, Y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# val_dataset = TensorDataset(C_hat_val_tensor, Y_val_tensor)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "482ea172",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4795, 5994]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m C_hat_train, C_hat_val, Y_train, Y_val = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_hat_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_Y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m X_train = torch.tensor(C_hat_train, dtype=torch.float32)\n\u001b[32m      4\u001b[39m Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2848\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_arrays == \u001b[32m0\u001b[39m:\n\u001b[32m   2846\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one array required as input\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2848\u001b[39m arrays = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2850\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2851\u001b[39m n_train, n_test = _validate_shuffle_split(\n\u001b[32m   2852\u001b[39m     n_samples, test_size, train_size, default_test_size=\u001b[32m0.25\u001b[39m\n\u001b[32m   2853\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/utils/validation.py:532\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    503\u001b[39m \n\u001b[32m    504\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    528\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    531\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/utils/validation.py:475\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    473\u001b[39m uniques = np.unique(lengths)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    478\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [4795, 5994]"
     ]
    }
   ],
   "source": [
    "C_hat_train, C_hat_val, Y_train, Y_val = train_test_split(C_hat_train, one_hot_Y_train, test_size=0.2, random_state=None)\n",
    "\n",
    "X_train = torch.tensor(C_hat_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_val = torch.tensor(C_hat_val, dtype=torch.float32)\n",
    "Y_val = torch.tensor(Y_val, dtype=torch.float32)\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "batch_size = 64\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_test = torch.tensor(C_hat_test, dtype=torch.float32, device=device)\n",
    "Y_test = torch.tensor(one_hot_Y_test, dtype=torch.float32, device=device)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "batch_size = 64\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a8c4e",
   "metadata": {},
   "source": [
    "## Learn Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb8f8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_concepts = 112\n",
    "num_classes = 200\n",
    "model = PrototypeClassifier(num_concepts, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lambda_binary = 0.01\n",
    "lambda_L1 = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7f50626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200]\n",
      "Train Loss: 65.5953, Train Accuracy: 76.12%, Validation Accuracy: 77.90%\n",
      "Epoch [20/200]\n",
      "Train Loss: 53.6675, Train Accuracy: 88.86%, Validation Accuracy: 89.66%\n",
      "Epoch [30/200]\n",
      "Train Loss: 43.0676, Train Accuracy: 91.51%, Validation Accuracy: 91.66%\n",
      "Epoch [40/200]\n",
      "Train Loss: 34.4253, Train Accuracy: 93.28%, Validation Accuracy: 93.33%\n",
      "Epoch [50/200]\n",
      "Train Loss: 27.7046, Train Accuracy: 93.91%, Validation Accuracy: 93.83%\n",
      "Epoch [60/200]\n",
      "Train Loss: 22.5938, Train Accuracy: 94.08%, Validation Accuracy: 93.83%\n",
      "Epoch [70/200]\n",
      "Train Loss: 18.7308, Train Accuracy: 94.10%, Validation Accuracy: 93.99%\n",
      "Epoch [80/200]\n",
      "Train Loss: 15.8056, Train Accuracy: 94.12%, Validation Accuracy: 93.99%\n",
      "Epoch [90/200]\n",
      "Train Loss: 13.5815, Train Accuracy: 94.10%, Validation Accuracy: 94.16%\n",
      "Epoch [100/200]\n",
      "Train Loss: 11.8813, Train Accuracy: 94.10%, Validation Accuracy: 94.08%\n",
      "Epoch [110/200]\n",
      "Train Loss: 10.5754, Train Accuracy: 94.04%, Validation Accuracy: 94.16%\n",
      "Epoch [120/200]\n",
      "Train Loss: 9.5650, Train Accuracy: 94.60%, Validation Accuracy: 95.33%\n",
      "Epoch [130/200]\n",
      "Train Loss: 8.7843, Train Accuracy: 94.68%, Validation Accuracy: 95.33%\n",
      "Epoch [140/200]\n",
      "Train Loss: 8.1754, Train Accuracy: 94.68%, Validation Accuracy: 95.41%\n",
      "Epoch [150/200]\n",
      "Train Loss: 7.7012, Train Accuracy: 94.70%, Validation Accuracy: 95.41%\n",
      "Epoch [160/200]\n",
      "Train Loss: 7.3316, Train Accuracy: 94.70%, Validation Accuracy: 95.41%\n",
      "Epoch [170/200]\n",
      "Train Loss: 7.0427, Train Accuracy: 94.66%, Validation Accuracy: 95.50%\n",
      "Epoch [180/200]\n",
      "Train Loss: 6.8154, Train Accuracy: 94.68%, Validation Accuracy: 95.50%\n",
      "Epoch [190/200]\n",
      "Train Loss: 6.6370, Train Accuracy: 94.70%, Validation Accuracy: 95.58%\n",
      "Epoch [200/200]\n",
      "Train Loss: 6.4983, Train Accuracy: 94.70%, Validation Accuracy: 95.66%\n"
     ]
    }
   ],
   "source": [
    "# train and test\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, lambda_binary, lambda_L1, device=device)\n",
    "    val_accuracy = val_epoch(model, val_loader)\n",
    "    if((epoch+1)%10==0):\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b82dc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6579219882637211"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels = Y_test.argmax(dim=1)\n",
    "predictions = model.predict(X_test)\n",
    "(predictions == real_labels).sum().item()/len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b09a36",
   "metadata": {},
   "source": [
    "# MY OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ade5c6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.88838958740234% of the values are close to 0 or 1\n"
     ]
    }
   ],
   "source": [
    "close_to_zero = (torch.sum((model.get_sigmoid_prototypes() < 0.1) | (model.get_sigmoid_prototypes() > 0.9)) / (200*112)).cpu().numpy()\n",
    "print(f\"{close_to_zero*100}% of the values are close to 0 or 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe9091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Plotting ---\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# epochs_range = range(1, epochs + 1)\n",
    "# plt.plot(epochs_range, train_losses, label='Training Loss', marker='o', linestyle='-')\n",
    "# plt.plot(epochs_range, val_losses, label='Validation Loss', marker='x', linestyle='--')\n",
    "# plt.title('Training and Validation Loss Over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Average Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Optional: Plot validation accuracy as well\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(epochs_range, val_accuracies, label='Validation Accuracy', marker='s', linestyle='-', color='green')\n",
    "# plt.title('Validation Accuracy Over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a987f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prototypes = []\n",
    "# for y in Y_train:\n",
    "#     prototypes.append(final_binary_prototypes[y])\n",
    "\n",
    "# prototypes = np.array(prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daad5b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to find the closest concept vector and predict the label\n",
    "# def predict_nearest_concept(instance, reference_concepts, reference_labels):\n",
    "#     distances = np.sum(np.abs(reference_concepts - instance), axis=1)\n",
    "#     min_idx = np.argmin(distances)\n",
    "#     return reference_labels[min_idx]\n",
    "\n",
    "# # Use prototypes as reference concepts and evaluate on C_hat_test\n",
    "# correct_predictions = 0\n",
    "# total_predictions = len(C_hat_test)\n",
    "\n",
    "# for i, test_instance in enumerate(C_hat_test):\n",
    "#     predicted_label = predict_nearest_concept(test_instance, prototypes, Y_train)\n",
    "#     true_label = Y_test[i]\n",
    "\n",
    "#     if predicted_label == true_label:\n",
    "#         correct_predictions += 1\n",
    "\n",
    "# # Calculate and print accuracy\n",
    "# accuracy = correct_predictions / total_predictions\n",
    "# print(f\"\\nOverall accuracy using prototype-based nearest neighbor: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666fb86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
