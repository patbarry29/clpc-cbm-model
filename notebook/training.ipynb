{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fa329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root_path = os.path.dirname(notebook_dir)\n",
    "sys.path.insert(0, project_root_path)\n",
    "\n",
    "from src import ModelXtoC\n",
    "from scripts.run_preprocessing import preprocessing_main\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f239f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11788 images.\n",
      "Processing in 369 batches of size 32 (for progress reporting)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|█████████████████████| 369/369 [00:55<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing.\n",
      "Successfully transformed: 11788 images.\n",
      "Found 11788 unique images.\n",
      "Found 312 unique concepts.\n",
      "Generated concept matrix with shape: (11788, 312)\n",
      "Found 200 classes.\n",
      "Found labels for 11788 images.\n",
      "Generated one-hot matrix with shape: (11788, 200)\n",
      "Split complete: 5994 train images, 5794 test images.\n",
      "Dataset initialized with 5994 pre-sorted items.\n",
      "Dataset initialized with 5794 pre-sorted items.\n"
     ]
    }
   ],
   "source": [
    "concept_labels, train_loader, val_loader, test_loader = preprocessing_main(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80140bb",
   "metadata": {},
   "source": [
    "# TRAINING IMPLEMENTATION (IN PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae643c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import find_class_imbalance\n",
    "from src import inception_v3\n",
    "from config import N_CONCEPTS, N_CLASSES\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5316e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                    else \"mps\" if torch.backends.mps.is_available()\n",
    "                    else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fc53a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Instantiated (X -> C)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = ModelXtoC(pretrained=True,\n",
    "                freeze=True,\n",
    "                n_classes=N_CLASSES, # Still needed for model structure, but won't be trained/used\n",
    "                use_aux=True,\n",
    "                n_concepts=N_CONCEPTS)\n",
    "\n",
    "model = model.to(device)\n",
    "print(\"Model Instantiated (X -> C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731e539",
   "metadata": {},
   "source": [
    "### Loss\n",
    "Original code from CBM Github uses a separate loss for each attribute\n",
    "\n",
    "`BCEWithLogitsLoss()` performs 2 steps:\n",
    "1. $\\sigma(x)$\n",
    "    - Applies the sigmoid function to the logits to get probabilities.\n",
    "2. $\\text{BCE}(\\sigma(x), y) = y \\cdot \\text{log}(\\sigma(x)) + (1-y) \\cdot (1-\\text{log}(\\sigma(x)))$\n",
    "    - Compute binary cross-entropy between output probabilities ($\\sigma(x)$) and ground truths ($y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be2ad09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weighted_loss = True # Set to False for simple unweighted loss\n",
    "\n",
    "if use_weighted_loss:\n",
    "    concept_weights = find_class_imbalance(concept_labels)\n",
    "    attr_criterion = [nn.BCEWithLogitsLoss(weight=torch.tensor([ratio], device=device, dtype=torch.float))\n",
    "                    for ratio in concept_weights]\n",
    "else:\n",
    "    attr_criterion = [nn.BCEWithLogitsLoss() for _ in range(N_CONCEPTS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c945c",
   "metadata": {},
   "source": [
    "### Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b50f189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer and Scheduler Ready\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "weight_decay = 0.00004\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                    lr=lr,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=weight_decay)\n",
    "\n",
    "scheduler_step = 1000 # Set large for almost constant LR, or smaller (e.g., 30, 50) for decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=0.1)\n",
    "\n",
    "print(\"Optimizer and Scheduler Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df0b28",
   "metadata": {},
   "source": [
    "### HELPERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e648c53",
   "metadata": {},
   "source": [
    "### Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "743f1447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 12.6132 | Acc: 86.639 (79.935) | Time: 64.12s\n",
      " Batch: 100/149 | Avg. Loss: 10.3432 | Acc: 87.430 (83.558) | Time: 39.07s\n",
      "Epoch 1 Train Summary | Loss: 9.3374 | Acc: 84.884\n",
      "Epoch 1 Val Summary   | Loss: 4.6143 | Acc: 89.454\n",
      "Current LR: 0.01\n",
      "--- Epoch 2/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 6.6800 | Acc: 87.059 (87.749) | Time: 44.21s\n",
      " Batch: 100/149 | Avg. Loss: 6.6678 | Acc: 87.440 (87.735) | Time: 39.46s\n",
      "Epoch 2 Train Summary | Loss: 6.5408 | Acc: 87.735\n",
      "Epoch 2 Val Summary   | Loss: 4.2268 | Acc: 89.561\n",
      "Current LR: 0.01\n",
      "--- Epoch 3/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 6.2529 | Acc: 88.732 (87.785) | Time: 43.94s\n",
      " Batch: 100/149 | Avg. Loss: 6.2263 | Acc: 87.740 (87.826) | Time: 39.50s\n",
      "Epoch 3 Train Summary | Loss: 6.2265 | Acc: 87.850\n",
      "Epoch 3 Val Summary   | Loss: 4.1397 | Acc: 89.664\n",
      "Current LR: 0.01\n",
      "--- Epoch 4/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 6.2427 | Acc: 88.492 (88.015) | Time: 44.12s\n",
      " Batch: 100/149 | Avg. Loss: 6.1054 | Acc: 87.320 (88.014) | Time: 39.53s\n",
      "Epoch 4 Train Summary | Loss: 6.0328 | Acc: 87.992\n",
      "Epoch 4 Val Summary   | Loss: 4.0720 | Acc: 89.720\n",
      "Current LR: 0.01\n",
      "--- Epoch 5/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 5.9393 | Acc: 88.191 (87.933) | Time: 43.27s\n",
      " Batch: 100/149 | Avg. Loss: 5.9479 | Acc: 87.169 (87.966) | Time: 39.26s\n",
      "Epoch 5 Train Summary | Loss: 5.8935 | Acc: 88.021\n",
      "Epoch 5 Val Summary   | Loss: 4.0200 | Acc: 89.783\n",
      "Current LR: 0.01\n",
      "--- Epoch 6/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 5.8241 | Acc: 88.401 (88.050) | Time: 44.13s\n",
      " Batch: 100/149 | Avg. Loss: 5.6912 | Acc: 87.690 (88.149) | Time: 39.90s\n",
      "Epoch 6 Train Summary | Loss: 5.7959 | Acc: 88.153\n",
      "Epoch 6 Val Summary   | Loss: 3.9399 | Acc: 89.841\n",
      "Current LR: 0.01\n",
      "--- Epoch 7/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 5.5068 | Acc: 88.171 (88.275) | Time: 43.82s\n",
      " Batch: 100/149 | Avg. Loss: 5.5681 | Acc: 88.672 (88.250) | Time: 39.31s\n",
      "Epoch 7 Train Summary | Loss: 5.6692 | Acc: 88.215\n",
      "Epoch 7 Val Summary   | Loss: 3.8901 | Acc: 89.882\n",
      "Current LR: 0.01\n",
      "--- Epoch 8/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 5.5705 | Acc: 89.032 (88.241) | Time: 45.11s\n",
      " Batch: 100/149 | Avg. Loss: 5.6129 | Acc: 88.331 (88.251) | Time: 39.89s\n",
      "Epoch 8 Train Summary | Loss: 5.5352 | Acc: 88.264\n",
      "Epoch 8 Val Summary   | Loss: 3.9135 | Acc: 89.930\n",
      "Current LR: 0.01\n",
      "--- Epoch 9/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 5.3802 | Acc: 88.431 (88.345) | Time: 43.97s\n",
      " Batch: 100/149 | Avg. Loss: 5.4852 | Acc: 88.211 (88.335) | Time: 39.74s\n",
      "Epoch 9 Train Summary | Loss: 5.4801 | Acc: 88.353\n",
      "Epoch 9 Val Summary   | Loss: 3.8624 | Acc: 90.000\n",
      "Current LR: 0.01\n",
      "--- Epoch 10/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 5.2633 | Acc: 89.213 (88.360) | Time: 47.76s\n",
      " Batch: 100/149 | Avg. Loss: 5.2971 | Acc: 88.321 (88.363) | Time: 44.32s\n",
      "Epoch 10 Train Summary | Loss: 5.3832 | Acc: 88.367\n",
      "Epoch 10 Val Summary   | Loss: 3.8371 | Acc: 90.036\n",
      "Current LR: 0.01\n",
      "--- Epoch 11/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 5.2020 | Acc: 89.143 (88.537) | Time: 50.12s\n",
      " Batch: 100/149 | Avg. Loss: 5.1876 | Acc: 88.031 (88.508) | Time: 44.80s\n",
      "Epoch 11 Train Summary | Loss: 5.3114 | Acc: 88.481\n",
      "Epoch 11 Val Summary   | Loss: 3.8470 | Acc: 90.073\n",
      "Current LR: 0.01\n",
      "--- Epoch 12/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 5.2961 | Acc: 88.281 (88.472) | Time: 49.36s\n",
      " Batch: 100/149 | Avg. Loss: 5.2538 | Acc: 88.592 (88.540) | Time: 45.14s\n",
      "Epoch 12 Train Summary | Loss: 5.2613 | Acc: 88.531\n",
      "Epoch 12 Val Summary   | Loss: 3.7991 | Acc: 90.122\n",
      "Current LR: 0.01\n",
      "--- Epoch 13/50 ---\n",
      " Batch: 50/149 | Avg. Loss: 5.1563 | Acc: 88.582 (88.477) | Time: 49.04s\n",
      " Batch: 100/149 | Avg. Loss: 5.1311 | Acc: 89.103 (88.542) | Time: 45.02s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m train_loss, train_acc = \u001b[43mrun_epoch_x_to_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_concepts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_CONCEPTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Train Summary | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/career/UJM Lab/hybrid-cbm-prototype-model/src/training/train.py:77\u001b[39m, in \u001b[36mrun_epoch_x_to_c\u001b[39m\u001b[34m(model, loader, criterion_list, optimizer, is_training, use_aux, n_concepts, device, log_interval)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[43mavg_batch_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     optimizer.step()\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Accuracy Calculation (using the main outputs collected)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "from src.training import run_epoch_x_to_c\n",
    "\n",
    "epochs = 50 # Adjust as needed\n",
    "log_interval = 50 # How often to print progress\n",
    "\n",
    "# print(\"\\nStarting Training Loop...\")\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_acc = run_epoch_x_to_c(model, train_loader, attr_criterion, optimizer, is_training=True, use_aux=True, n_concepts=N_CONCEPTS, device=device, log_interval=log_interval)\n",
    "    print(f'Epoch {epoch+1} Train Summary | Loss: {train_loss:.4f} | Acc: {train_acc:.3f}')\n",
    "\n",
    "    # Validate\n",
    "    if test_loader: # Assuming test_loader is your validation loader\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc = run_epoch_x_to_c(model, val_loader, attr_criterion, optimizer, is_training=False, use_aux=False, n_concepts=N_CONCEPTS, device=device, log_interval=log_interval)\n",
    "        print(f'Epoch {epoch+1} Val Summary   | Loss: {val_loss:.4f} | Acc: {val_acc:.3f}')\n",
    "\n",
    "#         # Save best model based on validation accuracy\n",
    "#         if val_acc > best_val_acc:\n",
    "#             print(f\"Validation accuracy improved ({best_val_acc:.3f} -> {val_acc:.3f}). Saving model...\")\n",
    "#             best_val_acc = val_acc\n",
    "#             torch.save(model.state_dict(), 'x_to_c_best_model.pth')\n",
    "#             print(\"Model saved to x_to_c_best_model.pth\")\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "    print(f\"Current LR: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "print(\"\\nTraining Finished.\")\n",
    "# Load best model for potential further use/testing\n",
    "model.load_state_dict(torch.load('x_to_c_best_model.pth'))\n",
    "print(\"Best model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00acb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
