{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64468ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root_path = os.path.dirname(notebook_dir)\n",
    "sys.path.insert(0, project_root_path)\n",
    "\n",
    "from src.models import ModelXtoC\n",
    "from src.preprocessing.CUB import preprocessing_main\n",
    "from src.utils import *\n",
    "from config import PROJECT_ROOT\n",
    "from src.training import run_epoch_x_to_c\n",
    "\n",
    "from src.utils import find_class_imbalance\n",
    "from config import CUB_CONFIG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60c4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIMMED_CONCEPTS = CUB_CONFIG['N_TRIMMED_CONCEPTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f39434-7c40-43b6-bde9-193b7f3b4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bff13a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11788 images.\n",
      "Processing in 369 batches of size 32 (for progress reporting)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 108/369 [00:20<00:49,  5.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m torch.manual_seed(\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m concept_labels, train_loader, test_loader = \u001b[43mpreprocessing_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_concepts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/career/UJM Lab/hybrid-cbm-prototype-model/src/preprocessing/CUB/preprocessing_main.py:22\u001b[39m, in \u001b[36mpreprocessing_main\u001b[39m\u001b[34m(class_concepts, verbose)\u001b[39m\n\u001b[32m     19\u001b[39m training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     20\u001b[39m mapping_file = os.path.join(PROJECT_ROOT, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCUB\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mimages.txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m image_tensors, _ = \u001b[43mload_and_transform_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# CREATE CONCEPT LABELS MATRIX\u001b[39;00m\n\u001b[32m     25\u001b[39m concept_labels_file = os.path.join(PROJECT_ROOT, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCUB\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mimage_concept_labels.txt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/career/UJM Lab/hybrid-cbm-prototype-model/src/preprocessing/CUB/image_processing.py:102\u001b[39m, in \u001b[36mload_and_transform_images\u001b[39m\u001b[34m(input_dir, mapping_file, resol, use_training_transforms, batch_size, verbose)\u001b[39m\n\u001b[32m     99\u001b[39m batch_paths = all_image_paths[i * batch_size : (i + \u001b[32m1\u001b[39m) * batch_size]\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m batch_paths:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# Apply transformations\u001b[39;00m\n\u001b[32m    104\u001b[39m     transformed_img_tensor = transform_pipeline(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/PIL/Image.py:984\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    982\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    988\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/PIL/ImageFile.py:250\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[32m    248\u001b[39m             \u001b[38;5;28mself\u001b[39m.map = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m err_code = -\u001b[32m3\u001b[39m  \u001b[38;5;66;03m# initialize to unknown error\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.map:\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# sort tiles in file order\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/PIL/ImageFile.py:326\u001b[39m, in \u001b[36mImageFile.load_prepare\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# create image memory if necessary\u001b[39;00m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28mself\u001b[39m.im = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m     \u001b[38;5;66;03m# create palette (optional)\u001b[39;00m\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "concept_labels, train_loader, test_loader = preprocessing_main(class_concepts=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2cf9e",
   "metadata": {},
   "source": [
    "**Find device to run model on (CPU or GPU).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5893fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                    else \"mps\" if torch.backends.mps.is_available()\n",
    "                    else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc3f7e",
   "metadata": {},
   "source": [
    "### Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f33d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weighted_loss = True # Set to False for simple unweighted loss\n",
    "\n",
    "if use_weighted_loss:\n",
    "    concept_weights = find_class_imbalance(concept_labels)\n",
    "    attr_criterion = [nn.BCEWithLogitsLoss(weight=torch.tensor([ratio], device=device, dtype=torch.float))\n",
    "                    for ratio in concept_weights]\n",
    "else:\n",
    "    attr_criterion = [nn.BCEWithLogitsLoss() for _ in range(N_TRIMMED_CONCEPTS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c567dd5-545b-4d38-ab3c-3a19a028a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs_as_array(outputs, n_concepts):\n",
    "    # Initialize an empty list to collect batches\n",
    "    batch_results = []\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        batch_size = outputs[i].shape[0]\n",
    "\n",
    "        # Create a batch matrix with N_CONCEPTS number of columns\n",
    "        batch_matrix = np.zeros((batch_size, n_concepts))\n",
    "\n",
    "        for instance_idx in range(batch_size):\n",
    "            # Extract, convert, and flatten data for the current concept\n",
    "            instance_data = outputs[i][instance_idx].detach().cpu().numpy().flatten()\n",
    "            batch_matrix[instance_idx, :] = instance_data\n",
    "\n",
    "        # Add this consistently shaped batch matrix to our collection\n",
    "        batch_results.append(batch_matrix)\n",
    "\n",
    "    return np.vstack(batch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7dda06",
   "metadata": {},
   "source": [
    "# Load instance-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(PROJECT_ROOT, 'models', 'CUB', 'instance_level_model.pth'), map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0856ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Summary   | Loss: 19.2846 | Acc: 81.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "if train_loader:\n",
    "    with torch.no_grad():\n",
    "        shuffled_concept_labels = []\n",
    "        shuffled_img_labels = []\n",
    "\n",
    "        # Iterate through all batches\n",
    "        for batch in train_loader:\n",
    "            _, concept_labels, image_labels, _ = batch\n",
    "            # Append batch labels to our list\n",
    "            shuffled_concept_labels.append(concept_labels)\n",
    "            shuffled_img_labels.append(image_labels)\n",
    "\n",
    "        # Concatenate all batches into a single tensor\n",
    "        shuffled_concept_labels = torch.cat(shuffled_concept_labels, dim=0)\n",
    "        shuffled_img_labels = torch.cat(shuffled_img_labels, dim=0)\n",
    "\n",
    "        test_loss, test_acc, outputs = run_epoch_x_to_c(\n",
    "            model, train_loader, attr_criterion, optimizer=None, n_concepts=N_TRIMMED_CONCEPTS, device=device,\n",
    "            return_outputs='sigmoid', verbose=True\n",
    "        )\n",
    "\n",
    "# print(f\"Shuffled labels shape: {shuffled_img_labels.shape}\")\n",
    "np.save(os.path.join(PROJECT_ROOT, 'output', 'C_train_instance.npy'), shuffled_concept_labels)\n",
    "np.save(os.path.join(PROJECT_ROOT, 'output', 'Y_train_instance.npy'), shuffled_img_labels)\n",
    "print(f'Best Model Summary   | Loss: {test_loss:.4f} | Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "912b1d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (5994, 112)\n"
     ]
    }
   ],
   "source": [
    "output_array = get_outputs_as_array(outputs, N_TRIMMED_CONCEPTS)\n",
    "print(f\"Final shape: {output_array.shape}\")\n",
    "\n",
    "np.save(os.path.join(PROJECT_ROOT, 'output', 'C_hat_sigmoid_train_instance.npy'), output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2583f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LOAD BEST MODEL FROM CBM PAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083c6d8-25bb-4662-b9f2-ec6f58152941",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = os.path.join(PROJECT_ROOT, 'models', 'CUB', 'best_model_1.pth')\n",
    "model = torch.load(best_model, map_location=device, weights_only=False)\n",
    "print(\"Best model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_loader:\n",
    "    with torch.no_grad():\n",
    "        shuffled_concept_labels = []\n",
    "        shuffled_img_labels = []\n",
    "\n",
    "        # Iterate through all batches\n",
    "        for batch in test_loader:\n",
    "            _, concept_labels, image_labels, _ = batch\n",
    "            # Append batch labels to our list\n",
    "            shuffled_concept_labels.append(concept_labels)\n",
    "            shuffled_img_labels.append(image_labels)\n",
    "\n",
    "        # Concatenate all batches into a single tensor\n",
    "        shuffled_concept_labels = torch.cat(shuffled_concept_labels, dim=0)\n",
    "        shuffled_img_labels = torch.cat(shuffled_img_labels, dim=0)\n",
    "\n",
    "        test_loss, test_acc, outputs = run_epoch_x_to_c(\n",
    "            model, test_loader, attr_criterion, optimizer=None, n_concepts=N_TRIMMED_CONCEPTS, device=device,\n",
    "            return_outputs='sigmoid', verbose=True\n",
    "        )\n",
    "\n",
    "# print(f\"Shuffled labels shape: {shuffled_img_labels.shape}\")\n",
    "np.save(os.path.join(PROJECT_ROOT, 'output', 'C_test.npy'), shuffled_concept_labels)\n",
    "np.save(os.path.join(PROJECT_ROOT, 'output', 'Y_test.npy'), shuffled_img_labels)\n",
    "print(f'Best Model Summary   | Loss: {test_loss:.4f} | Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524e965-29d4-4eab-8903-ba107b4b3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array = get_outputs_as_array(outputs, N_TRIMMED_CONCEPTS)\n",
    "np.save(os.path.join(PROJECT_ROOT, 'output', 'C_hat_sigmoid_test.npy'), output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10bfded-b36b-4e47-a63f-80e066e54ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = os.path.join(PROJECT_ROOT, 'models', 'CUB', 'best_model_2.pth')\n",
    "# model = torch.load(best_model, map_location=device, weights_only=False)\n",
    "# print(\"Best model loaded.\")\n",
    "\n",
    "# if train_loader:\n",
    "#     with torch.no_grad():\n",
    "#         test_loss, test_acc = run_epoch_x_to_c(model, train_loader, attr_criterion, optimizer, n_concepts=N_TRIMMED_CONCEPTS, device=device)\n",
    "\n",
    "# print(f'Best Model Train Summary   | Loss: {test_loss:.4f} | Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d541c-fff5-473f-9857-ab7b6bbcf80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if test_loader:\n",
    "#     with torch.no_grad():\n",
    "#         test_loss, test_acc = run_epoch_x_to_c(model, test_loader, attr_criterion, optimizer, n_concepts=N_TRIMMED_CONCEPTS, device=device)\n",
    "\n",
    "# print(f'Best Model Test Summary    | Loss: {test_loss:.4f} | Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274dc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = os.path.join(PROJECT_ROOT, 'models', 'CUB', 'best_model_3.pth')\n",
    "# model = torch.load(best_model, map_location=device, weights_only=False)\n",
    "# print(\"Best model loaded.\")\n",
    "\n",
    "# if train_loader:\n",
    "#     with torch.no_grad():\n",
    "#         test_loss, test_acc = run_epoch_x_to_c(model, train_loader, attr_criterion, optimizer, n_concepts=N_TRIMMED_CONCEPTS, device=device)\n",
    "\n",
    "# print(f'Best Model Train Summary   | Loss: {test_loss:.4f} | Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4289f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if test_loader:\n",
    "#     with torch.no_grad():\n",
    "#         test_loss, test_acc = run_epoch_x_to_c(model, test_loader, attr_criterion, optimizer, n_concepts=N_TRIMMED_CONCEPTS, device=device)\n",
    "\n",
    "# print(f'Best Model Test Summary    | Loss: {test_loss:.4f} | Acc: {test_acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
